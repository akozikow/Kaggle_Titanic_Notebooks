{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ffd2511-ae5e-48df-a582-ae0cc101f313",
   "metadata": {},
   "source": [
    "## Welcome to my workbook for the Kaggle Titanic Competition using a neural newtork developed with Pytorch!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e47edf1-2a2d-4a2b-8db0-c31095c90404",
   "metadata": {},
   "source": [
    "To begin, we will have to download the training and test datasets, which we will download programmatically with the Kaggle API. Note: you will have to have a valid kaggle API token (kaggle.json file in your '.kaggle' folder) to use the below code to download the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "90a5e5ac-479b-4a3a-b172-bcd0bc3085ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaggle imported!\n",
      "titanic.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "try:\n",
    "    import kaggle\n",
    "    print('kaggle imported!')\n",
    "except:\n",
    "    !pip install kaggle\n",
    "    import kaggle\n",
    "    print('kaggle installed and imported!')\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "kaggle.api.authenticate()\n",
    "kaggle.api.competition_download_files('titanic', quiet = False)\n",
    "with zipfile.ZipFile('titanic.zip', 'r') as zip_file:\n",
    "    zip_file.extractall('extracted_files/')\n",
    "train_file = pd.read_csv('extracted_files/train.csv')\n",
    "test_file = pd.read_csv('extracted_files/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991c134b-9f95-4c5b-8ce8-9b3e5fef1354",
   "metadata": {},
   "source": [
    "Now that we have our data, we will begin preparing it. The EDA has been done in another Jupyter notebook on my Github called 'Kaggle_Titanic_Competition', so we will skip that component and apply the insights we found there to this dataset and prepare it accordingly; this will include dropping unhelpful features and imputing missing values on ones we intend to keep. We will also change the data type of Pclass to an object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e33ff94b-4036-432e-a2b4-76326264c2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "train_target = train_file[['Survived']]\n",
    "train_data = train_file\n",
    "train_data = train_data.drop(['PassengerId', 'Survived', 'Name', 'Ticket', 'Cabin'], axis = 1)\n",
    "imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n",
    "e_imputer = SimpleImputer(missing_values = np.nan, strategy = 'most_frequent')\n",
    "train_data[['Age']] = imputer.fit_transform(train_data[['Age']])\n",
    "train_data[['Embarked']] = e_imputer.fit_transform(train_data[['Embarked']])\n",
    "train_data['Pclass'] = train_data['Pclass'].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fbecf363-f2bb-4c10-967b-cdd3bdea03a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_file\n",
    "test_data = test_data.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis = 1)\n",
    "imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n",
    "e_imputer = SimpleImputer(missing_values = np.nan, strategy = 'most_frequent')\n",
    "test_data[['Age']] = imputer.fit_transform(test_data[['Age']])\n",
    "test_data[['Embarked']] = e_imputer.fit_transform(test_data[['Embarked']])\n",
    "test_data[['Fare']] = imputer.fit_transform(test_data[['Fare']])\n",
    "test_data['Pclass'] = test_data['Pclass'].astype('object')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5f5dee-bea9-491c-ba45-29c8ba8c1d51",
   "metadata": {},
   "source": [
    "Next, we will get the names of object and numerical columns so that we can split them and apply a OneHotEncoder and StandardScaler respectively before recombining them into a new DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7a673d38-3fc3-4b23-aa2a-8bd9fd8654e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Age', 'SibSp', 'Parch', 'Fare']\n",
      "['Pclass', 'Sex', 'Embarked']\n"
     ]
    }
   ],
   "source": [
    "num_columns = train_data.select_dtypes(include = np.number).columns.to_list()\n",
    "obj_columns = train_data.select_dtypes(include = 'object').columns.to_list()\n",
    "num_train_data = train_data[num_columns]\n",
    "obj_train_data = train_data[obj_columns]\n",
    "print(num_columns)\n",
    "print(obj_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a426d720-e627-42e7-ac08-7288059a1d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akozi\\AppData\\Local\\Temp\\ipykernel_28456\\3049080054.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  num_train_data[['Age']] = scaler.fit_transform(num_train_data[['Age']])\n",
      "C:\\Users\\akozi\\AppData\\Local\\Temp\\ipykernel_28456\\3049080054.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  num_train_data[['Fare']] = scaler.fit_transform(num_train_data[['Fare']])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "num_train_data[['Age']] = scaler.fit_transform(num_train_data[['Age']])\n",
    "num_train_data[['Fare']] = scaler.fit_transform(num_train_data[['Fare']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "df1eb5e8-3c68-4af8-93e6-264cc951433d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "obj_train_data\n",
    "obj_train_data = encoder.fit_transform(obj_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "20121a18-7f43-4506-b4e2-c7862706a04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = encoder.get_feature_names_out(obj_columns)\n",
    "obj_train_data = pd.DataFrame(obj_train_data, columns = names)\n",
    "train_concat = pd.concat([num_train_data, obj_train_data], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0c7116-0564-4062-9445-fb1de8b751cf",
   "metadata": {},
   "source": [
    "With the prepared training data, we can split our data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4753797e-a73d-460e-b55a-8fc5d3f60d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_concat, train_file['Survived'], random_state = 42, test_size = 0.2)\n",
    "X_train_t = torch.tensor(X_train.to_numpy()).to(torch.float32)\n",
    "X_test_t = torch.tensor(X_test.to_numpy()).to(torch.float32)\n",
    "y_train_t = torch.tensor(y_train.to_numpy()).to(torch.float32)\n",
    "y_test_t = torch.tensor(y_test.to_numpy()).to(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef869a9a-eb54-4050-a9e8-2fb98398b4bb",
   "metadata": {},
   "source": [
    "Now that the training and test sest are prepared, we can start to develop our neural network. For this network, I used three linear layers with a ReLU layer seperating them; I tested the performance of the neural network empirically and found the below architecture below to provide the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0b4e86f7-4f31-430b-9200-25d254579012",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanicRegressor(nn.Module):\n",
    "  def __init__(self, in_features, out_features, hidden_units):\n",
    "    super().__init__()\n",
    "    self.layer_stack = nn.Sequential(\n",
    "        nn.Linear(in_features = in_features, out_features = hidden_units),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features = hidden_units, out_features = hidden_units),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features = hidden_units, out_features = out_features)\n",
    "    )\n",
    "  def forward(self, x):\n",
    "    return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa0fadf-9bf3-4af9-8876-963b5ef70081",
   "metadata": {},
   "source": [
    "Below we instantiate a model using the architecture we developed and dictate parameters for the network and the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7c84a06d-ac00-4757-b48c-3b83407b9f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = TitanicRegressor(in_features= 12, out_features = 1, hidden_units = 32)\n",
    "optimizer = torch.optim.Adam(params = nn_model.parameters(), lr = 0.001)\n",
    "loss_fn = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c507ce8d-bf40-46ae-8f75-2b70d43673a4",
   "metadata": {},
   "source": [
    "Below, we perform the training and test loops for our neural network, which will tell us how effectively it is reducing error across its "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3f834f63-77df-4720-8329-06cb2161c34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.6690300107002258 | test loss 0.6732843518257141\n",
      "train loss 0.6508817672729492 | test loss 0.6573548316955566\n",
      "train loss 0.6295585632324219 | test loss 0.6378886699676514\n",
      "train loss 0.6038028001785278 | test loss 0.6126531958580017\n",
      "train loss 0.5719224810600281 | test loss 0.5800976753234863\n",
      "train loss 0.5355101227760315 | test loss 0.5423466563224792\n",
      "train loss 0.4994131326675415 | test loss 0.5036120414733887\n",
      "train loss 0.4690190553665161 | test loss 0.4690033495426178\n",
      "train loss 0.4473961889743805 | test loss 0.44419610500335693\n",
      "train loss 0.43310123682022095 | test loss 0.43085962533950806\n",
      "train loss 0.422170490026474 | test loss 0.4249804615974426\n",
      "train loss 0.4134940207004547 | test loss 0.4234435558319092\n",
      "train loss 0.40654322504997253 | test loss 0.4232333302497864\n",
      "train loss 0.40030142664909363 | test loss 0.42333459854125977\n",
      "train loss 0.3945460319519043 | test loss 0.42390334606170654\n",
      "train loss 0.38949185609817505 | test loss 0.42578816413879395\n",
      "train loss 0.3850749135017395 | test loss 0.427647203207016\n",
      "train loss 0.3813599944114685 | test loss 0.4291946291923523\n",
      "train loss 0.3780726194381714 | test loss 0.43072184920310974\n",
      "train loss 0.3749753534793854 | test loss 0.4325639605522156\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "  nn_model.train()\n",
    "  optimizer.zero_grad()\n",
    "  preds = nn_model(X_train_t).squeeze()\n",
    "  loss = loss_fn(preds, y_train_t)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  nn_model.eval()\n",
    "  with torch.inference_mode():\n",
    "    test_preds = nn_model(X_test_t).squeeze()\n",
    "    test_loss = loss_fn(test_preds, y_test_t)\n",
    "    if epoch % 10 == 0:\n",
    "      print(f'train loss {loss} | test loss {test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1769994-f19d-42f1-9e1f-3e327d1889d0",
   "metadata": {},
   "source": [
    "After the training and testing loops which have optimized the weights and baises of the neural networks, we will convert the test logits into predictions by applying a sigmoid function followed by rounding them to 0 or 1 and finally displaying the accuracy (where the predictions and truth are equal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "19109f6c-fe7f-4e00-9958-13d58e299c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8268)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = torch.round(torch.sigmoid(test_preds)) == y_test_t\n",
    "accuracy = predictions.sum()/torch.numel(predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76bf412-ceb1-43c8-970c-cff34010512e",
   "metadata": {},
   "source": [
    "We have trained and evaluated our model. Now we wil prepare the test data the same way we prepared our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "499c648c-084a-4247-9d11-07597f4a1baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 12 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Age         418 non-null    float64\n",
      " 1   SibSp       418 non-null    int64  \n",
      " 2   Parch       418 non-null    int64  \n",
      " 3   Fare        418 non-null    float64\n",
      " 4   Pclass_1    418 non-null    float64\n",
      " 5   Pclass_2    418 non-null    float64\n",
      " 6   Pclass_3    418 non-null    float64\n",
      " 7   Sex_female  418 non-null    float64\n",
      " 8   Sex_male    418 non-null    float64\n",
      " 9   Embarked_C  418 non-null    float64\n",
      " 10  Embarked_Q  418 non-null    float64\n",
      " 11  Embarked_S  418 non-null    float64\n",
      "dtypes: float64(10), int64(2)\n",
      "memory usage: 39.3 KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akozi\\AppData\\Local\\Temp\\ipykernel_28456\\3139695106.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  num_test_data[['Age']] = scaler.fit_transform(num_test_data[['Age']])\n",
      "C:\\Users\\akozi\\AppData\\Local\\Temp\\ipykernel_28456\\3139695106.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  num_test_data[['Fare']] = scaler.fit_transform(num_test_data[['Fare']])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "num_test_data = test_data[num_columns]\n",
    "obj_test_data = test_data[obj_columns]\n",
    "scaler = StandardScaler()\n",
    "num_test_data[['Age']] = scaler.fit_transform(num_test_data[['Age']])\n",
    "num_test_data[['Fare']] = scaler.fit_transform(num_test_data[['Fare']])\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "obj_test_data = encoder.fit_transform(obj_test_data)\n",
    "names = encoder.get_feature_names_out(obj_columns)\n",
    "obj_test_data = pd.DataFrame(obj_test_data, columns = names)\n",
    "test_concat = pd.concat([num_test_data, obj_test_data], axis = 1)\n",
    "test_concat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d9755cda-53e5-4c99-a27c-7d3561e581cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([418, 12])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_final = torch.tensor(test_concat.to_numpy()).to(torch.float32)\n",
    "X_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10518584-528d-4208-8b10-5833ca5e5547",
   "metadata": {},
   "source": [
    "Finally, we will make predictions on the blinded testing data. The line to create a csv file with the precictions is commented out. Thank you for reviewing my notebook; I am early in my journey and a lot to learn about Data Science, and this was my first attempt at creating a neural network from scratch on data I had cleaned and prepared myself, so I found the process highly educational. Ultimately, this neural network performed just a bit worse than the XGBoost Classifier I trained for the same purpose in my companion notebook on my GitHub, though i suspect with more data the neural network would eventually outperform the XGBoost Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "58d84dfe-ce85-4396-b761-ecee0dee4e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([418])\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "nn_model.eval()\n",
    "with torch.inference_mode():\n",
    "    test_preds_final = nn_model(X_final).squeeze()\n",
    "    final_predictions = torch.round(torch.sigmoid(test_preds_final))\n",
    "print(final_predictions.shape)\n",
    "final_predictions = final_predictions.numpy()\n",
    "pred_df = pd.DataFrame({'PassengerId' : test_file['PassengerId'], 'Survived':final_predictions})\n",
    "pred_df[['Survived']] = pred_df[['Survived']].astype('int64')\n",
    "# pred_df.to_csv('akozikowski_titanic_preds_nn.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
